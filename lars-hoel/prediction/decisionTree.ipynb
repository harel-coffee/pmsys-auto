{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pathg to injury_ts.csv\n",
    "injuryPath = \"\"\n",
    "\n",
    "#Path to Session dataset\n",
    "sessionPath = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot_and_fill_csv_file(csv_file_path, interpolate=False):\n",
    "   \n",
    "    # read the csv file without header\n",
    "    df = pd.read_csv(csv_file_path, header=None)\n",
    "\n",
    "    # Get the player names from the first row of the dataframe and remove the first 6 characters\n",
    "    player_names = [name[6:] for name in df.iloc[0, 1:]]\n",
    "\n",
    "    # Drop the first row since it contains player names\n",
    "    df = df.drop(0)\n",
    "\n",
    "    # Reset the index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Set the player names as column names\n",
    "    df.columns = ['Date'] + player_names\n",
    "\n",
    "    # Melt the dataframe to unpivot the player columns\n",
    "    df_melt = df.melt(id_vars=['Date'], var_name='Player_name', value_name='Injured')\n",
    "\n",
    "    # Sort the dataframe by player name and date\n",
    "    df_melt = df_melt.sort_values(by=['Player_name', 'Date'])\n",
    "\n",
    "    # Reset the index\n",
    "    df_melt = df_melt.reset_index(drop=True)\n",
    "\n",
    "    if interpolate:\n",
    "        # Iterate through the DataFrame and update 0s to 1s if the day before and day after are 1s\n",
    "        df_melt['Injured'] = df_melt['Injured'].astype(int)\n",
    "        for i in range(1, len(df_melt) - 1):\n",
    "            if df_melt.loc[i - 1, 'Injured'] == 1 and df_melt.loc[i + 1, 'Injured'] == 1:\n",
    "                df_melt.loc[i, 'Injured'] = 1\n",
    "\n",
    "    return df_melt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_for_team(team_name, interpolate=False):\n",
    "    \"\"\"\n",
    "    Creates a dataframe with the data from Session_calc and injury_ts for one team\n",
    "    \"\"\"\n",
    "\n",
    "    # read csv files into dataframes\n",
    "    df1 = pd.read_csv(sessionPath)\n",
    "    df2 = unpivot_and_fill_csv_file(injuryPath, interpolate)\n",
    "    df2['Date'] = pd.to_datetime(df2['Date'], format='%d.%m.%Y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    if team_name == \"TeamA\":\n",
    "        filtered_df1 = df1[df1['Team_Name'] != 'TeamB']\n",
    "        filtered_df2 = df2[~df2['Player_name'].str.startswith('TeamB')]\n",
    "    else:\n",
    "        filtered_df1 = df1[df1['Team_Name'] != 'TeamA']\n",
    "        filtered_df2 = df2[~df2['Player_name'].str.startswith('TeamA')]\n",
    "\n",
    "    # Find the minimum and maximum dates in df1\n",
    "    min_date = pd.to_datetime(df1['Date']).min().strftime('%Y-%m-%d')\n",
    "    max_date = pd.to_datetime(df1['Date']).max().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Filter df2 based on the minimum and maximum dates in df1\n",
    "    filtered_df2 = filtered_df2[(filtered_df2['Date'] >= min_date) & (filtered_df2['Date'] <= max_date)]\n",
    "\n",
    "    merged_df = pd.merge(filtered_df1, filtered_df2, on=['Date', 'Player_name'], how='outer')\n",
    "\n",
    "    # add all the data from df2 that is not already in the new dataframe\n",
    "    new_df = pd.concat([merged_df, filtered_df2[~filtered_df2.set_index(['Date', 'Player_name']).index.isin(merged_df.set_index(['Date', 'Player_name']).index)]])\n",
    "    merged = new_df[['Date', 'Player_name', 'Duration', 'Total_distance', 'Average_running_speed', 'Top_speed', 'Metabolic_power', 'HIR', 'Injured']]\n",
    "    merged = merged.dropna(subset=['Injured'])  # remove rows where Injured is NaN\n",
    "    merged = merged.sort_values(by='Date')  # sort by Date\n",
    "    merged = merged.fillna(0)  # replace NaN with 0\n",
    "\n",
    "    \n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining class imbalce functions\n",
    "\n",
    "#Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def oversample_data(X_train, y_train, sampling_ratio=0.35):\n",
    "\n",
    "    # Apply oversampling on the training data\n",
    "    sampling_ratio = 0.35\n",
    "    ros = RandomOverSampler(sampling_strategy=sampling_ratio, random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "    X_train_resampled = X_resampled.reshape(X_resampled.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Calculate the percentage of 0s after oversampling\n",
    "    zero_percentage_after = np.sum(y_resampled == 0) / len(y_resampled) * 100\n",
    "    #print(f\"Percentage of 0s after oversampling: {zero_percentage_after:.2f}%\")\n",
    "\n",
    "    return X_train_resampled, y_resampled, zero_percentage_after\n",
    "\n",
    "\n",
    "# Undersampling\n",
    "def undersample_data(X_train, y_train, sampling_ratio=0.35):\n",
    "\n",
    "    # Apply undersampling on the training data\n",
    "    sampling_strategy = 1 - sampling_ratio\n",
    "    rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "    X_train_resampled = X_resampled.reshape(X_resampled.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Calculate the percentage of 0s after undersampling\n",
    "    zero_percentage_after = np.sum(y_resampled == 0) / len(y_resampled) * 100\n",
    "\n",
    "    return X_train_resampled, y_resampled, zero_percentage_after\n",
    "\n",
    "\n",
    "#SMOTE\n",
    "def smote_data(X_train, y_train, sampling_ratio):\n",
    "\n",
    "    # Set a custom sampling ratio\n",
    "    sampling_ratio = 0.35\n",
    "\n",
    "    # Apply SMOTE with the custom ratio\n",
    "    smote = SMOTE(sampling_strategy=sampling_ratio, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "    # Reshape the resampled data\n",
    "    X_train_resampled = X_resampled.reshape(X_resampled.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Calculate the percentage of 0s after oversampling\n",
    "    zero_percentage_after = np.sum(y_resampled == 0) / len(y_resampled) * 100\n",
    "\n",
    "\n",
    "    return X_train_resampled, y_resampled, zero_percentage_after\n",
    "\n",
    "\n",
    "# ADASYN\n",
    "def adasyn_oversample_data(X_train, y_train, sampling_ratio=0.35):\n",
    "\n",
    "    # Apply ADASYN oversampling on the training data\n",
    "    sampling_strategy = sampling_ratio\n",
    "    adasyn = ADASYN(sampling_strategy=sampling_strategy, random_state=42, n_neighbors=5)\n",
    "    X_resampled, y_resampled = adasyn.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "\n",
    "    X_train_resampled = X_resampled.reshape(X_resampled.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Calculate the percentage of 0s after oversampling\n",
    "    zero_percentage_after = np.sum(y_resampled == 0) / len(y_resampled) * 100\n",
    "    \n",
    "\n",
    "    return X_train_resampled, y_resampled, zero_percentage_after\n",
    "\n",
    "\n",
    "#Data sample mode\n",
    "def sample_mode(X_train, y_train, sampling_ratio, oversample_mode):\n",
    " precentageOfZeroesInDataset = (y_train == 0).sum() / len(y_train)\n",
    "\n",
    " if oversample_mode == 'oversample':\n",
    "      X_train, y_train, precentageOfZeroesInDataset = oversample_data(X_train, y_train, sampling_ratio)\n",
    " elif oversample_mode == 'undersample':\n",
    "      X_train, y_train, precentageOfZeroesInDataset = undersample_data(X_train, y_train, sampling_ratio)\n",
    " elif oversample_mode == 'smote':\n",
    "      X_train, y_train, precentageOfZeroesInDataset = smote_data(X_train, y_train, sampling_ratio)\n",
    " elif oversample_mode == 'adasyn':\n",
    "      X_train, y_train, precentageOfZeroesInDataset = adasyn_oversample_data(X_train, y_train, sampling_ratio)\n",
    " \n",
    "    \n",
    " return X_train, y_train, precentageOfZeroesInDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrtix that shows injuries as TP\n",
    "def confusion_matrix_only_injuries(y_test, y_pred):\n",
    "    tp, fn, fp, tn = 0, 0, 0, 0\n",
    "    for i, j in zip(y_test, y_pred):\n",
    "        if i == 1 and j == 1:\n",
    "            tp += 1\n",
    "        elif i == 1 and j == 0:\n",
    "            fn += 1\n",
    "        elif i == 0 and j == 1:\n",
    "            fp += 1\n",
    "        elif i == 0 and j == 0:\n",
    "            tn += 1\n",
    "    # Create a DataFrame for the confusion matrix\n",
    "    confusion_matrix = pd.DataFrame(\n",
    "        {\n",
    "            \"Predicted Injuries\": [tp, fp],\n",
    "            \"Predicted Non-Injuries\": [fn, tn],\n",
    "        },\n",
    "        index=[\"Actual Injuries\", \"Actual Non-Injuries\"]\n",
    "    )\n",
    "\n",
    "    return confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datapreprocessing\n",
    "\n",
    "def convert_duration_to_seconds(duration):\n",
    "    if str(duration) == \"0\": return 0\n",
    "    \n",
    "    time_parts = str(duration).split(':')\n",
    "    hours, minutes, seconds = int(time_parts[0]), int(time_parts[1]), int(time_parts[2])\n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "    return total_seconds\n",
    "\n",
    "def create_player_sequences(df, player, sequence_length=7):\n",
    "    player_df = df[df['Player_name'] == player]\n",
    "    player_data = player_df.drop(['Date', 'Player_name'], axis=1).values\n",
    "\n",
    "    sequences = []\n",
    "    for i in range(len(player_data) - sequence_length):\n",
    "        sequences.append(player_data[i:i + sequence_length])\n",
    "\n",
    "    return np.array(sequences)\n",
    "\n",
    "def create_team_sequence(df, sequence_length):\n",
    "    players = df['Player_name'].unique()\n",
    "    all_sequences = []\n",
    "    for player in players:\n",
    "        player_sequences = create_player_sequences(df, player, sequence_length)\n",
    "        all_sequences.append(player_sequences)\n",
    "\n",
    "    data = np.concatenate(all_sequences, axis=0)\n",
    "    X, y = data[:, :-1, :], data[:, -1, -1]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess(inputWindow, interpolate=False):\n",
    "    # Load your dataframe\n",
    "    df = merge_data_for_team(\"TeamA\", interpolate=interpolate)\n",
    "\n",
    "    # Convert 'Duration' column to seconds\n",
    "    df['Duration'] = df['Duration'].apply(convert_duration_to_seconds)\n",
    "\n",
    "    # Preprocess the data\n",
    "    X, y = create_team_sequence(df, inputWindow)\n",
    "\n",
    "    # Convert data types\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.float32)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "\n",
    "def decision_tree_classification(test_size, oversample_mode, sampling_ratio):\n",
    "    # Preprocess the data\n",
    "    X_scaled = TimeSeriesScalerMinMax().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=42, stratify=y)\n",
    "    precentageOfZeroesInDataset = (y_train == 0).sum() / len(y_train)\n",
    "\n",
    "    precentageOfZeroesInDataset = np.sum(y_train == 0) / len(y_train) * 100\n",
    "    X_train, y_train, precentageOfZeroesInDataset = sample_mode(X_train, y_train, sampling_ratio, oversample_mode)\n",
    "\n",
    "    # Reshape the data for Decision Tree\n",
    "    X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # Train the model\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train_2d, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test_2d)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    confInjuries = confusion_matrix_only_injuries(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, f1, precision, recall, confInjuries, precentageOfZeroesInDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple runs\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# List of hyperparameters\n",
    "input_windows = [4, 7, 30]\n",
    "test_sizes = [0.2, 0.4]\n",
    "oversample_modes = [\"none\", \"oversample\", \"undersample\", \"smote\", \"adasyn\"]\n",
    "sampling_ratios = [0.2, 0.4]\n",
    "interploate_injuries = [True, False]\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through all combinations of hyperparameters\n",
    "for interplate in interploate_injuries:\n",
    "    for preprocess_param in input_windows:\n",
    "        X, y = preprocess(preprocess_param)\n",
    "        \n",
    "        for test_size_param in test_sizes:\n",
    "            for oversample_mode in oversample_modes:\n",
    "                for sampling_ratio in sampling_ratios:\n",
    "                    \n",
    "                    accuracy, f1, precision, recall, confusion, precentageOfZeroesInDataset = decision_tree_classification(test_size=test_size_param,\n",
    "                                                                                    oversample_mode=oversample_mode,\n",
    "                                                                                    sampling_ratio=sampling_ratio)\n",
    "\n",
    "                    # Save the results\n",
    "                    results.append({\"input_windows\": preprocess_param,\n",
    "                                    \"Interplate\": interplate,\n",
    "                                    \"test_size\": test_size_param,\n",
    "                                    \"oversample_mode\": oversample_mode,\n",
    "                                    \"sampling_ratio\": sampling_ratio,\n",
    "                                    \"Prectenage of zeroes in dataset\": precentageOfZeroesInDataset,\n",
    "                                    \"accuracy\": accuracy,\n",
    "                                    \"f1\": f1,\n",
    "                                    \"precision\": precision,\n",
    "                                    \"recall\": recall,\n",
    "                                    \"confusion\": confusion})\n",
    "\n",
    "# Sort the results based on the F1 score\n",
    "results.sort(key=lambda x: x['f1'], reverse=True)\n",
    "\n",
    "# Print the top 3 performers\n",
    "for i in range(min(3, len(results))):\n",
    "    print(f\"Top Performer {i + 1}:\")\n",
    "    print(f\"Input window: {results[i]['input_windows']}\")\n",
    "    print(f\"Interplate: {results[i]['Interplate']}\")\n",
    "    print(f\"Test Size: {results[i]['test_size']}\")\n",
    "    print(f\"Oversample Mode: {results[i]['oversample_mode']}\")\n",
    "    print(f\"Sampling Ratio: {results[i]['sampling_ratio']}\")\n",
    "    print(f\"Percentage of zeroes in dataset: {results[i]['Prectenage of zeroes in dataset']}\")\n",
    "    print(f\"Accuracy: {results[i]['accuracy']}\")\n",
    "    print(f\"F1: {results[i]['f1']}\")\n",
    "    print(f\"Precision: {results[i]['precision']}\")\n",
    "    print(f\"Recall: {results[i]['recall']}\")\n",
    "    display(results[i]['confusion'])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
